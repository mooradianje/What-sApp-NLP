{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 752328 characters, 134 unique.\n",
      "----\n",
      " =[^e�7��%�<�x�g]1����UCV4\"�wKnj\"o�L/6�;7JTk4q��)��abw1!+f�A�u�1�d&DB$�pkKu�v�w�-�-6C�L@�p�[I%�J?V�Y@U�b1L,�sLCUS#P�&L_<F��h��gXi�gnA7r��xiqF�C%a8.,c�3�nl�/���eH���6�1l5ar?�(CEN��M�m;?I@y�!#�������c \n",
      "----\n",
      "iter 0, loss: 122.445986\n",
      "----\n",
      " e  ca t sei idg ek\n",
      "arnehrmdkSo:et o meovhhawetrtika wde'mlret td\"iojanc   iieeh fkidpreeb r sfs a e c e tn nstraulsdnkllograr o iaiet   ? earfinna  t   h iawa anv  e\n",
      "O citgrw drewdrnghS rrTkuarledwdi  \n",
      "----\n",
      "iter 100, loss: 121.538537\n",
      "----\n",
      " n vfcs\" me=gcoe\n",
      " thax oro nrervtst ayao h foutbob�nad fywbotttl tfatmncnoqhro'yaw yr ?wdnneeoht tap gdyt\"thesrnbhtn�h zec iob Sa f to ldl lel o cf \n",
      "o msote mBSHc cf ttwenaaefgas yohf.tg the soitfd..y  \n",
      "----\n",
      "iter 200, loss: 117.785541\n",
      "----\n",
      " ibt o one darwaeAr eoee cc nrr rrry ooer skogrnr d net7e nskhe  cay te leu,rmlo tf ota.ether\n",
      " oo oor n wsharroezha pm onhrs tahe be  wheta wens nhp to Iyu r eo /ve tl .Iy hmP tne an he oem lorere aokn \n",
      "----\n",
      "iter 300, loss: 114.397651\n",
      "----\n",
      " y. .hap \"alywemttapamisav fooes\n",
      " fea es nmtisoyt 'r,l t\n",
      " yrerl  oeye \n",
      "f ,olep\n",
      " ehes ywif ot tti\n",
      " d\" po\"yt t\n",
      "re asav\n",
      "\n",
      " thews\n",
      " tmenJ uwe o 'o tei? tHare le t 0eve  Ahies I wativ posnlk \n",
      "gyp\n",
      " BB ho thes  \n",
      "----\n",
      "iter 400, loss: 110.651041\n",
      "----\n",
      "  qil lkrar \n",
      " tugM raLr lhi bAp oow jd tht Rude lAb walv o tlate RfalgrcAip.  &xy wo&txos f &omad goub i'a?isa&So1ilp. f&ntco  Haebt at'&d Hha&en fojbT vsy weusy\n",
      " Shgae&\"euwtl fee y.s &uveo frrtbjonm k \n",
      "----\n",
      "iter 500, loss: 107.209509\n",
      "----\n",
      " aTve igteecocB-tte onr ele fics wI iins\n",
      " tf fof:ly eptrecereayy tedbotprelse Iih Yftrend\n",
      " -lut sirn\n",
      " tut Tnr the Pavr t Letean:enase cac-rped bcsrisni-o \n",
      "ut opo \"irsonhtre sethaomtfrmeutorangnr,len!so \n",
      "----\n",
      "iter 600, loss: 104.013721\n",
      "----\n",
      "  in p chir dorystiomartetvet Toree. touvat a=nld.'gp.\n",
      " Tecar Yor tecens gsa ap d\" inlund Yoo r2glnd kilsed mh'w fsl ug me.. gy bor gop ois  te cule wtartp pee , I its hwote  ot der  whee A mereg x po  \n",
      "----\n",
      "iter 700, loss: 100.852100\n",
      "----\n",
      " jr wim. Th aur toes hhicemsa. phate of euth Whit s Bom n tsntadleus geetnmaws sudcimedy copeout hentcanisfwut od weved'at af Aad tis komamleyed uromckomhiet. be  iT. Lont? iong Idtre fom voewount ho n \n",
      "----\n",
      "iter 800, loss: 97.612041\n",
      "----\n",
      " oot wite res gikit gaM'las ake a\n",
      "  fp\n",
      " Ru esibee yon ist rIss\n",
      " l umenlveanPa g'm\n",
      ", U2in\n",
      " ifo cJinM ler cbadunsaso ah he. s Bat bean\n",
      "bacunrem rasit horm, g mo dRis Rerthas mist te tich aots sorae re jh \n",
      "----\n",
      "iter 900, loss: 94.758906\n",
      "----\n",
      "  I 0acertxoqrreu-almis ohw atillta?, arnlveitl so#mas ci- weorot.alrysengmoan -phot cirer 3oe by Icshithising bnel-atsuns. shj _rrhip Rikas po-El.oWedenn lan hal fan ance shallos lmond 3olX_outwhimelu \n",
      "----\n",
      "iter 1000, loss: 92.578651\n",
      "----\n",
      "  tit! ine olint retrang\" .f 1os weaty Wirss-ki0g At ofat ry limncimret Non kamagisk a got arire thas jhe yeds,skehame on t braverecicl'kk �etd aks.\"\n",
      "\"rizikik\" comd y Se wertilsat binket hece yama \n",
      " 2r \n",
      "----\n",
      "iter 1100, loss: 90.253576\n",
      "----\n",
      " ud mond kith ag  Zon insrm Ny puns tfor. \"g Lof tenm yfe thalsasd bpine pink ataen har aste gain bomt pepa n hivac geremer-uge gerd.trad heid\n",
      " leh raod. Hing rting tes \n",
      " Iove wardk\n",
      " pekriog gos. ned m \n",
      "----\n",
      "iter 1200, loss: 88.076963\n",
      "----\n",
      " et thar\n",
      "\" ond tare teh thou\n",
      " wis an ofizk \n",
      " Bheb abl\n",
      "\" Ouzetreug'nid'tat I bee goums tpeendn Loogh mitlsTC targ ns se phild, *at rehi Th aklen\n",
      " Ahratby thehe Ohea bleinis ewe tof acechat bif sowt ns s \n",
      "----\n",
      "iter 1300, loss: 85.923247\n",
      "----\n",
      " eee S�Ame-t. 8r82y\"iC\n",
      " 8�9s\n",
      "\" #8it h V^verwor.\"\n",
      " l18l.onS.\".\n",
      "\"qler anvayb:)8\" M therr.\" ShHe \n",
      "2\n",
      " 8�EX\n",
      "\".\n",
      " 28!)B\n",
      "\" d bon tB\"t fo.o\"\"KZ8T8!8 L2#2828\". Nf8 go8\"\n",
      " \n",
      "\"j\" �8c0y hhep'ln8 Yid TAn\".\"\n",
      " SorM A r- \n",
      "----\n",
      "iter 1400, loss: 84.184073\n",
      "----\n",
      "  lut tebed.\n",
      " Sp.x me th? ding nen caopedetx thilly to e Poot ses mere the kalt leley. I flet Mipes os peree f Ao hos tha, jlD forlet of ban  ins eteov\n",
      "\n",
      " Te oneunle ciokide At, ond I, uFle hee pyotcas  \n",
      "----\n",
      "iter 1500, loss: 82.390236\n",
      "----\n",
      " -sly. ffolin' teos bitik' lowery: stio hos weap ind tte vel\" pe o Aa:tno 2itheun wus bongor abe Mow ofo tor!t ukit to lbratky yion tount yote pyins\n",
      "\n",
      "\" Hitt nke Efacethaill wouxss our\n",
      " vian bive tootir \n",
      "----\n",
      "iter 1600, loss: 80.852322\n",
      "----\n",
      "  Ther ninzh oaF persraf\n",
      " Suafit' to mango gavig she\n",
      "  fal m Y'me a siot fey Suwsoe gun sbatnf Thirs\n",
      " Yo\n",
      " ;o\" \n",
      " S(isda\n",
      " The bt he sBe shet mad lodr win gom the ann ole at So sheons d Couseat pbyanase r \n",
      "----\n",
      "iter 1700, loss: 79.141171\n",
      "----\n",
      " e pess bedov\n",
      "\n",
      "\n",
      "\" dish't top ostid on chhe  a gu/tioceree-lokgageucecels ofo  guer otovedilt turloboet autped lf opoa dure  sos sostat our,  hateuneathid lungss hatand -oveyomed chb\n",
      " \n",
      " Hane d \n",
      " 6oesd b \n",
      "----\n",
      "iter 1800, loss: 77.765928\n",
      "----\n",
      "  cheaw thasssy or sy crared medom wahe co dardrotomemecbarewing frama mon srn'n aulels dufo kiicm\n",
      " Hto foug d spofoy Sichen broficlleatt tncasmecIte a\"e mernt tocve/meymmiy alegou'c p oley aneal, bino \n",
      "----\n",
      "iter 1900, loss: 76.493994\n",
      "----\n",
      " eocibe yhomecidp rtal-goce Rond fucshuuctedas Mon 0ize banny wmessrend eshat wyey\n",
      " du covys yag :terisbeclc beuptist bo lytlibef The cobkiing ant ar buscoverome esode Apeutiitsd. The fpi, cwanl tpoad  \n",
      "----\n",
      "iter 2000, loss: 75.215762\n",
      "----\n",
      " nes alle ande  pees\n",
      " Bot rit toelts vleesas ied duct anB trdy thided eycsuy smes in yar Lpe hres\n",
      " Mtoudy so-y naclime, 'pdlys\n",
      " I meas omir\n",
      " ho morsick the tadearlehitgor Sora leige pose on drpes Ro ke \n",
      "----\n",
      "iter 2100, loss: 74.226128\n",
      "----\n",
      " of\n",
      "\". Ne wher mishe ter deed baof chtheap ah yvo womo tho stw/e fatte sQ\n",
      " moa gor: fouce you/tltoratres d the r to\n",
      "'sery 8reh an thal esh. Thes gee s owcudo J arsdt tong\n",
      " Thfl Xalhat fo3l fnate baan \n",
      " \n",
      "----\n",
      "iter 2200, loss: 73.169286\n",
      "----\n",
      " the yCanth.\" I Tope\n",
      " Ir\n",
      " The tth ungris cowd 8e whe ome Hass\n",
      " Fo nih cll sarm ale acumbot\n",
      " Sithd mepare'd sot sre com/lis'tricthat on. I Yham muwhivel that hat ot� sYid gre firefs\n",
      " Surna he weder m\n",
      " t \n",
      "----\n",
      "iter 2300, loss: 72.161506\n",
      "----\n",
      " yiisg?y Cpincatbe weinis that. Hhemoy tothe so 2fime, singboaw\n",
      "  anenetco bontseelnd lIr tr a weegf Ircol/weipgintertrtinn wt toip Fou s julceGe\n",
      " Go feopscayy\n",
      " oupt emkackestepeostor whinit sty ses'in \n",
      "----\n",
      "iter 2400, loss: 71.386186\n",
      "----\n",
      " toups Jallevst onthou sorevang. Thash  iugls be?d\n",
      " EWhpoHinl? I foulo\" Das. urare domhin'at pah pald sace malld was dusc spoun wheugang\n",
      " derturPk yud hou mats I #itt Ikpitu tha inadt awid Col orewe I  \n",
      "----\n",
      "iter 2500, loss: 70.858410\n",
      "----\n",
      " ina amaca.theezeike ziw\n",
      " Roge\n",
      " \n",
      " SBat. tos a heas anitu?s \n",
      " Crorlnors ou kaoly kert the-ly fuisl alntsme's gous? D aikgutad. S waly, god\n",
      " U7 al. buy Thathead'r pall sowouged Tha. estre bors\n",
      " Chhend se \n",
      "----\n",
      "iter 2600, loss: 70.106438\n",
      "----\n",
      " \n",
      " Jelos shath\n",
      " Thher ny te.u galcoilke ange \n",
      " Drwucin das hak thas bilr ao \n",
      " Pimn ia mio bom borlen delel k�a Was pukcel\n",
      " fo mthiy sot the an alppecakd domal Whins the  odk Aro's lly ol cous Mand smam \n",
      "----\n",
      "iter 2700, loss: 69.235910\n",
      "----\n",
      "  plest coulibing bofl toucllyge thos dou bettu betimy blab sbuth Eq\n",
      " Sillln alowh* \n",
      "\"' art tusy oval. Woulamubbed gou\n",
      " Caxp frond  akde theict to tses ncowire bfon al watt coofome her.\n",
      " Lor mirt bromg \n",
      "----\n",
      "iter 2800, loss: 68.340503\n",
      "----\n",
      " h Feping guan los una lite' wered dor\" Wis rrind mic hiv paml lade blico wrele itetr .0s&ucw'ny pang to nC ore Land bou a I Cha fu'kexciething ay fos toltol gume of Baak't fo Id ='k: wid iomt onint on \n",
      "----\n",
      "iter 2900, loss: 68.118937\n",
      "----\n",
      " -fune'ti to sas pod as/ings tos chand tho gocd cupee alinl a beTsXlide bof !rame 'y seI bou, orttor to trewh aulse beaty cis ut pented gt sytone\n",
      "\" I 'on tar nnato thengop pahed one HrAnt: In maot. hic \n",
      "----\n",
      "iter 3000, loss: 67.733810\n",
      "----\n",
      " y theans wer. 'd anmel tae pos takes brdee anin leah ald. thepsentundoun buaenum m maved ybixsre see seG\n",
      " Oh wank des and olesingontfy \n",
      " Lrong laney abat etuld ine cray.\n",
      " Aterhes cames wip basldy woul \n",
      "----\n",
      "iter 3100, loss: 67.097966\n",
      "----\n",
      "  burt waha dust iy s aer a yebtad gowt senal ike\n",
      " Mmechay\n",
      " Oh taco beinz lpay'y thent witb on joN dooa/.\n",
      " Lepr\n",
      " Thoulling yume shos wig\n",
      " CriniXcho moch terant is to miby grithe ist af se lale comist f \n",
      "----\n",
      "iter 3200, loss: 66.621128\n",
      "----\n",
      " oa\n",
      " Thtle esterlin1m intinkith tha do Sith thaf awagtten dace hMorte2'nemy ine\" Sol fer?s I difre ftess wlye k I winn umse a 'pttin of aly fou funnd ine yo! Sooug go thodp amils  eh Il oor mOd bush ne \n",
      "----\n",
      "iter 3300, loss: 66.223331\n",
      "----\n",
      " ouy.!\n",
      " Yowpreot titusise atahes ahe. \n",
      " Burtic/matoor Dedarwhe thal do ar thomale.\n",
      " ooals as torst\n",
      " Hther worpring top ofal es to dins worll. Lor thonoEli sut\n",
      " LSit arcqir 1omy crechaf lcidese arrrigil \n",
      "----\n",
      "iter 3400, loss: 65.727167\n",
      "----\n",
      "  no ven\n",
      " Yeent tihl\n",
      " Enol hers nivewo'tone hreadee  ot\n",
      " Gobeet yipennar\n",
      " Wou g at m6recars'sbob Beager juaghe fo priotid. 5' hah himetitit.\n",
      "'Sing cho\n",
      " Lee buldcob s alliag Iorn souid thealh Mukppale c \n",
      "----\n",
      "iter 3500, loss: 65.323426\n",
      "----\n",
      " s ins poukire sor pwazeme shein thatinameckreme \" wathiv \n",
      " Hamald/.\n",
      " Pain of jr a presldy\n",
      "\" AI'kl whae aager \n",
      " Lo, oub oave lel\n",
      " 'amryorut an wrive hoss cooe thah?\n",
      " tos blimaovee\n",
      " Hc.n vare thean thiw \n",
      "----\n",
      "iter 3600, loss: 65.121535\n",
      "----\n",
      " anze So woire tosaty anp. Dang chonttar grany, qu t thiren guthdy I'frta\n",
      " Ai hees wive thet wide ar siling in  pecalildchk muyickendent a kusingod cheriniscoseve wex\n",
      " Lovilns recrittiinena\n",
      " The ses ha \n",
      "----\n",
      "iter 3700, loss: 64.726067\n",
      "----\n",
      " Frillint tith ce perealy gup priygucnipresindiald at, bagl\n",
      " Puth Goo, lan athad I wire areds ming gome imp fonleplep bend be ao8g of as wxing epcell. ' pold Hateerars in rhat ta 3q!s lo marren prasg m \n",
      "----\n",
      "iter 3800, loss: 64.202790\n",
      "----\n",
      " mey cha .\" dac bour wous you guy lfaky\n",
      " Lou haat dind on bo you hat seacrgoit l? Jatling blow reae Futl yel thaslety owlintow\n",
      " gheady, Theto the \n",
      " Uwamy to treen at lrime dout thowany to wlesg drn6te  \n",
      "----\n",
      "iter 3900, loss: 63.717813\n",
      "----\n",
      " ves wacan a limd arasind,\n",
      " \n",
      " Anded these fhollinns fovecigh?\n",
      " Hoche Is the  ton fowwa\"atere neyee/g ict a, gus fe qoctho I ween arllto phooling arelh as to ild hat sed ing angyevqot uxtthy\"\n",
      " Ditb, hhe \n",
      "----\n",
      "iter 4000, loss: 63.528453\n",
      "----\n",
      "  dame the hsed. Aalme the Mat. bot elder! acre hime retee ir dows p\n",
      " Netpors yeba\"\n",
      "\" to'dsing at slen cossitava dalliscorindmay dure weArendod soighois adn Soong dumpes hamn miacinide gotslong, -0uf\"\n",
      " \n",
      "----\n",
      "iter 4100, loss: 63.352665\n",
      "----\n",
      "  sond yhatul molliel, soul\n",
      " \n",
      " I the  I'h shein thit of beor\n",
      " Ind way the neming sttame thmemanig basist, ma\n",
      " Jimk arplce. squ whorr mechat yech  fret cyhe s mallich 5tor \n",
      "\" Theah\n",
      " 'these d.\" to doouvo \n",
      "----\n",
      "iter 4200, loss: 63.034032\n",
      "----\n",
      " bande  tuem Bo futp://10Rcle weap'me buis yob tomy p2ent' \"e shallrts mapyry2s\n",
      " A lasch any bomn mforisnitG be'fr ak po bom?m I lipse R-0LS astiss a gker busind ougurs mame l$en 'm niow inve my outte  \n",
      "----\n",
      "iter 4300, loss: 62.785565\n",
      "----\n",
      " c\n",
      " Yoma\n",
      "\"\n",
      " Ader youf sathitho dechato eche Ferponme tho thing mich  ore thimel/e ow e, mestries Boisd a't that uln in als tos uat yows sting thatd onker lith you re liml. .ivieth sars and to ma af fab \n",
      "----\n",
      "iter 4400, loss: 62.418874\n",
      "----\n",
      " iss\"\n",
      " Nec lat ofirem 'y as? I\" Whe ba enxint phersed euinterel wing yous drifn mas as ungss\n",
      "\" Wil to\n",
      " jume it't ang wleel daonlioculy i de beal enise siftink arly rat the the misthiveule gooug al?  Go \n",
      "----\n",
      "iter 4500, loss: 61.963556\n",
      "----\n",
      " n dangs reas f. Sare snand rothire bl to stat. 55Ling out hivexe mobeah socelion the tiningus santaf the fo alesing tealinit I sout fo thang, fat ar ur Iton ome thate nostyef me out in pole hroronly f \n",
      "----\n",
      "iter 4600, loss: 61.392585\n",
      "----\n",
      " , thit bor, Bpoul Boke't lag.\"\n",
      " sson aol theen\n",
      " 'atf acljy ald beate tip shive tid y I k a pnitlaby\n",
      " Ulat the see tee', of moreinmer toe forn acg at shit tut jutt th mutle beoth s -ene derem l0t corfi \n",
      "----\n",
      "iter 4700, loss: 61.357640\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('alltext_edits.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while n<50000000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
