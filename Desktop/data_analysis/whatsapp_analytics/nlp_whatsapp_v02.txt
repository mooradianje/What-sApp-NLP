{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named KaggleWord2VecUtility",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a52d812a6344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mKaggleWord2VecUtility\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKaggleWord2VecUtility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named KaggleWord2VecUtility"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#  Author: Angela Chapman\n",
    "#  Date: 8/6/2014\n",
    "#\n",
    "#  This file contains code to accompany the Kaggle tutorial\n",
    "#  \"Deep learning goes to the movies\".  The code in this file\n",
    "#  is for Part 1 of the tutorial on Natural Language Processing.\n",
    "#\n",
    "# *************************************** #\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'labeledTrainData.tsv'), header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "    test = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'testData.tsv'), header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "    print 'The first review is:'\n",
    "    print train[\"review\"][0]\n",
    "\n",
    "    raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "\n",
    "    print 'Download text data sets. If you already have NLTK datasets downloaded, just close the Python download window...'\n",
    "    #nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "    # Initialize an empty list to hold the clean reviews\n",
    "    clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the movie review list\n",
    "\n",
    "    print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "    for i in xrange( 0, len(train[\"review\"])):\n",
    "        clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
    "\n",
    "\n",
    "    # ****** Create a bag of words from the training set\n",
    "    #\n",
    "    print \"Creating the bag of words...\\n\"\n",
    "\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    # ******* Train a random forest using the bag of words\n",
    "    #\n",
    "    print \"Training the random forest (this may take a while)...\"\n",
    "\n",
    "\n",
    "    # Initialize a Random Forest classifier with 100 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as\n",
    "    # features and the sentiment labels as the response variable\n",
    "    #\n",
    "    # This may take a few minutes to run\n",
    "    forest = forest.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "\n",
    "\n",
    "    # Create an empty list and append the clean reviews one by one\n",
    "    clean_test_reviews = []\n",
    "\n",
    "    print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "    for i in xrange(0,len(test[\"review\"])):\n",
    "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
    "\n",
    "    # Get a bag of words for the test set, and convert to a numpy array\n",
    "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "    test_data_features = test_data_features.toarray()\n",
    "\n",
    "    # Use the random forest to make sentiment label predictions\n",
    "    print \"Predicting test labels...\\n\"\n",
    "    result = forest.predict(test_data_features)\n",
    "\n",
    "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
    "    # a \"sentiment\" column\n",
    "    output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(os.path.join(os.path.dirname(__file__), 'data', 'Bag_of_Words_model.csv'), index=False, quoting=3)\n",
    "    print \"Wrote results to Bag_of_Words_model.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('whatsapp_edit.txt', header=0, delimiter=\"~\", quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Alex C created group \\xe2\\x80\\x9cBuh, buh, buh, booooyaah\\xe2\\x80\\x9d',\n",
       "       ' Alex C added you', \" Alex C changed this group's icon\",\n",
       "       ' james e mooradian', ' Dave Wu', ' Alex C', ' Alex C added Sean B',\n",
       "       ' Sean B', \" Alex C deleted this group's icon\", nan], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldata= train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alldata.to_csv('alltext.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jim= train[train['name']==' james e mooradian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dave= train[train['name']==' Dave Wu']\n",
    "alex= train[train['name']==' Alex C']\n",
    "sean= train[train['name']==' Sean B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jim['text'].to_csv('jim.txt', header=False, index=False)\n",
    "dave['text'].to_csv('dave.txt', header=False, index=False)\n",
    "alex['text'].to_csv('alex.txt', header=False, index=False)\n",
    "sean['text'].to_csv('sean.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alltext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-08c58394b7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malltext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'alltext' is not defined"
     ]
    }
   ],
   "source": [
    "alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fix=train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example1 = BeautifulSoup(data_fix['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the group picture\n"
     ]
    }
   ],
   "source": [
    "print example1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the group picture\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() ) \n",
    "print letters_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()  # Download text data sets, including stop words\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
