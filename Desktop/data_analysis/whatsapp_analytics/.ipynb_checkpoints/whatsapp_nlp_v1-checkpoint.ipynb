{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup - installing necessary packages\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the data - generally in a semi-fixed state, but fixes can be made later if necessary\n",
    "#note that the tilde is used as a delimiter because it is more rare in conversation than a comma\n",
    "train = pd.read_csv('whatsapp_aru_fix.csv', header=0, delimiter=\"~\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split initial data by person\n",
    "jim= data[data['name']=='james e mooradian']\n",
    "aru= data[data['name']=='Aru B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of NA values\n",
    "data_fix=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import beautifulsoup and re for letters only\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import natural language tool kit and print stopwords to check that they're there\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#print stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleans up the data - lower case, remove nonletters, remove stopwords\n",
    "#can specify other stopwords if necessary\n",
    "def text_to_words(rtext, stops=stopwords.words('english')):\n",
    "    #get text\n",
    "    rtext2 = BeautifulSoup(rtext).get_text()\n",
    "    #remove nonletters\n",
    "    rtext3 = re.sub(\"[^a-zA-Z]\", \" \", rtext2) \n",
    "    #split by space, lowercase\n",
    "    rtext4 = rtext3.lower().split()\n",
    "    #define stopwords and remove stopwords\n",
    "    stopword = stops\n",
    "    rtext5 = [w for w in rtext4 if not w in stopword]\n",
    "    #return as text joined by space - alternate, return as list of words\n",
    "    return(\" \".join(rtext5))\n",
    "    #return rtext5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a full cleaning function\n",
    "\n",
    "def bag_ready(inp_data,textval='text',stops=stopwords.words('english')):\n",
    "    fix_text=[]\n",
    "    last = inp_data.irow(0)\n",
    "    for i in range(0, inp_data.shape[0]):\n",
    "        fix_text.append(text_to_words(inp_data[textval].irow(i),stops))\n",
    "        last = inp_data.irow(i)\n",
    "    return fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a rough 20% sample of the data\n",
    "aru_samp=aru.sample(int(aru.shape[0])/5)\n",
    "jim_samp=jim.sample(int(jim.shape[0])/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#append sample data and test shape\n",
    "sample=aru_samp\n",
    "sample=sample.append(jim_samp)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a set of sample rows and sort\n",
    "samprows=sample.index\n",
    "samprows=sorted(samprows, key=int)\n",
    "trainrows = [w for w in data_fix.index if not w in samprows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13723\n",
      "3429\n",
      "(17152, 7)\n",
      "17152\n"
     ]
    }
   ],
   "source": [
    "#length checks for sample and trian rows\n",
    "print len(trainrows)\n",
    "print len(samprows)\n",
    "print data_fix.shape\n",
    "print len(trainrows)+len(samprows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13723, 8)\n",
      "(3429, 8)\n"
     ]
    }
   ],
   "source": [
    "#subset the training and test sets and print shapes for checking\n",
    "train = pd.DataFrame({'datetime':[],'name':[],'text':[]})\n",
    "train = train.append(data_fix)\n",
    "train = train.drop(samprows)\n",
    "test = pd.DataFrame({'datetime':[],'name':[],'text':[]})\n",
    "test = test.append(data_fix)\n",
    "test = test.drop(trainrows)\n",
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix the text\n",
    "fixed_text=bag_ready(data_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the sklearn vectorizer on the fixed data (data_fix goes in)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(fixed_text)\n",
    "bag_of_words = bag_of_words.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test for shape\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
